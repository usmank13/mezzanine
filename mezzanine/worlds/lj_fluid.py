"""Lennard–Jones (LJ) fluid / droplet *world* adapter.

This adapter loads the HDF5 file produced by `examples/lj_generate_dataset.py`.

The world instances are **single configurations** (one MD snapshot) with fields:
  {
    "pos": np.ndarray[N,3] float32,
    "box": float32,                # cubic box length L in reduced units
    "label": int,                  # chosen by cfg.label_field
    "state_id": int,               # index of (rho,T) state point
    "phase": int,                  # 0=vapor-like, 1=droplet present (cluster)
    "rho": float,
    "T": float,
    "meta": {...}
  }

The intent is that MD-symmetry transformations (SE(3), particle permutation,
periodic image choice) are treated as **symmetries**.
"""

from __future__ import annotations

from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional

import json
import numpy as np
import h5py

from ..core.cache import hash_dict
from ..core.deterministic import deterministic_subsample_indices
from .base import WorldAdapter
from ..registry import ADAPTERS


LabelField = Literal["state_id", "phase"]


@dataclass
class LJFluidH5AdapterConfig:
    """Configuration for loading an LJ dataset from HDF5."""

    path: str

    # Which stored label to expose as `label`.
    label_field: LabelField = "state_id"

    # Deterministic subsampling
    n_train: int = 20000
    n_test: int = 5000
    seed: int = 0

    # Optional: filter to a subset of state points (by id)
    state_ids: Optional[List[int]] = None

    def validate(self) -> None:
        if not self.path:
            raise ValueError("path is required")
        if self.n_train <= 0 or self.n_test <= 0:
            raise ValueError("n_train and n_test must be > 0")
        if self.label_field not in ("state_id", "phase"):
            raise ValueError(f"Unsupported label_field: {self.label_field}")


@ADAPTERS.register(
    "lj_fluid_h5",
    description="Load an LJ fluid/droplet snapshot dataset from HDF5 (generated by examples/lj_generate_dataset.py).",
)
class LJFluidH5Adapter(WorldAdapter):
    NAME = "lj_fluid_h5"
    DESCRIPTION = "Lennard–Jones fluid/droplet snapshots from an HDF5 dataset."

    def __init__(self, cfg: LJFluidH5AdapterConfig):
        cfg.validate()
        self.cfg = cfg

    def fingerprint(self) -> str:
        d = asdict(self.cfg)
        d["__class__"] = self.__class__.__name__
        p = Path(self.cfg.path)
        try:
            st = p.stat()
            d["file_size"] = int(st.st_size)
            d["file_mtime"] = int(st.st_mtime)
        except Exception:
            d["file_size"] = None
            d["file_mtime"] = None
        # Include generator meta hash if available
        try:
            with h5py.File(p, "r") as h5:
                meta_json = h5.attrs.get("meta_json", "")
            d["meta_hash"] = hash_dict({"meta_json": str(meta_json)})
        except Exception:
            d["meta_hash"] = None
        return hash_dict(d)

    def _load_split(self, h5: h5py.File, split: str) -> Dict[str, np.ndarray]:
        grp = h5[split]
        out: Dict[str, np.ndarray] = {}
        # Required fields
        for k in ["pos", "box", "state_id", "phase", "rho", "T"]:
            if k not in grp:
                raise KeyError(f"Dataset missing '{split}/{k}'")
            out[k] = grp[k][...]
        # Optional
        if "rep" in grp:
            out["rep"] = grp["rep"][...]
        return out

    def load(self) -> Dict[str, Any]:
        p = Path(self.cfg.path)
        if not p.exists():
            raise FileNotFoundError(
                f"LJ dataset not found: {p}. Generate it first with: python examples/lj_generate_dataset.py --out {p}"
            )

        with h5py.File(p, "r") as h5:
            meta_json = str(h5.attrs.get("meta_json", "{}"))
            meta = json.loads(meta_json) if meta_json else {}
            tr = self._load_split(h5, "train")
            te = self._load_split(h5, "test")

        # Optional state_id filter
        state_ids = None
        if self.cfg.state_ids is not None and len(self.cfg.state_ids) > 0:
            state_ids = set(int(x) for x in self.cfg.state_ids)

        def build_examples(
            arrs: Dict[str, np.ndarray], *, split: str, n_target: int, split_seed: int
        ) -> List[Dict[str, Any]]:
            n_total = int(arrs["pos"].shape[0])
            if state_ids is None:
                idx_pool = np.arange(n_total)
            else:
                idx_pool = np.nonzero(np.isin(arrs["state_id"], list(state_ids)))[0]
            if n_target > len(idx_pool):
                raise ValueError(
                    f"Requested n={n_target} from split with pool size {len(idx_pool)} after filtering. "
                    f"Increase dataset size or reduce n_train/n_test/state_ids."
                )
            sub = deterministic_subsample_indices(len(idx_pool), n_target, split_seed)
            idx = idx_pool[sub]

            label_arr = arrs[self.cfg.label_field]

            out: List[Dict[str, Any]] = []
            for k in idx.tolist():
                ex = {
                    "pos": arrs["pos"][k].astype(np.float32, copy=False),
                    "box": float(arrs["box"][k]),
                    "label": int(label_arr[k]),
                    "state_id": int(arrs["state_id"][k]),
                    "phase": int(arrs["phase"][k]),
                    "rho": float(arrs["rho"][k]),
                    "T": float(arrs["T"][k]),
                    "meta": {
                        "split": str(split),
                        "dataset_path": str(p),
                    },
                }
                if "rep" in arrs:
                    ex["rep"] = int(arrs["rep"][k])
                out.append(ex)
            return out

        train = build_examples(
            tr,
            split="train",
            n_target=int(self.cfg.n_train),
            split_seed=int(self.cfg.seed) + 17,
        )
        test = build_examples(
            te,
            split="test",
            n_target=int(self.cfg.n_test),
            split_seed=int(self.cfg.seed) + 19,
        )

        # Include useful meta
        n_atoms = int(train[0]["pos"].shape[0]) if len(train) > 0 else None
        return {
            "train": train,
            "test": test,
            "meta": {
                "adapter": self.NAME,
                "cfg": asdict(self.cfg),
                "generator_meta": meta,
                "n_atoms": n_atoms,
            },
        }
